{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf994e0",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f1aea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datasets import Dataset, DatasetDict\n",
    "cwd = os.getcwd()\n",
    "pol_dir = cwd+\"/src/d01_data\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c16fa",
   "metadata": {},
   "source": [
    "## Exploring original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a212d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Policydesigncharacteristics': 11380, 'Technologyandapplicationspecificity': 5989, 'Instrumenttypes': 3487})\n",
      "Counter({'Actor': 6021, 'InstrumentType': 3388, 'TechnologySpecificity': 2526, 'EnergySpecificity': 1908, 'Compliance': 1829, 'ApplicationSpecificity': 1550, 'Reference': 1181, 'Objective': 992, 'Time': 798, 'Resource': 511, 'InstrumentType_2': 98, 'Reversibility': 40, 'end': 14})\n",
      "Counter({'Addressee_default': 2081, 'Form_monitoring': 1781, 'Tech_LowCarbon': 1468, 'Energy_LowCarbon': 1358, 'Addressee_sector': 1356, 'RegulatoryInstr': 1231, 'Unspecified': 1100, 'Tech_Other': 1058, 'Authority_default': 939, 'App_Other': 786, 'App_LowCarbon': 764, 'Ref_OtherPolicy': 737, 'Authority_monitoring': 659, 'Objective_QualIntention': 556, 'Energy_Other': 550, 'Addressee_monitored': 483, 'Resource_Other': 362, 'Authority_legislative': 330, 'Edu_Outreach': 329, 'Time_Monitoring': 323, 'Ref_Strategy_Agreement': 319, 'FrameworkPolicy': 318, 'Time_Compliance': 310, 'Objective_QualIntention_noCCM': 266, 'Objective_QuantTarget': 162, 'TradablePermit': 146, 'Addressee_resource': 142, 'Ref_PolicyAmended': 125, 'Resource_MonSpending': 113, 'Time_InEffect': 109, 'Subsidies_Incentives': 102, 'PublicInvt': 98, 'TaxIncentives': 78, 'VoluntaryAgrmt': 72, 'Form_sanctioning': 48, 'Time_PolDuration': 42, 'Reversibility_policy': 40, 'Resource_MonRevenues': 36, 'Authority_established': 31, '': 14, 'Time_Resources': 14, 'RD_D': 12, 'Objective_QuantTarget_noCCM': 8})\n",
      "20856\n",
      "['Compliance', 'TechnologySpecificity', 'Reference', 'Objective', 'InstrumentType_2', 'end', 'EnergySpecificity', 'Actor', 'InstrumentType', 'Time', 'Resource', 'Reversibility', 'ApplicationSpecificity'] 13\n",
      "['', 'Authority_monitoring', 'Tech_Other', 'Unspecified', 'Time_Resources', 'Authority_default', 'Resource_MonSpending', 'Ref_OtherPolicy', 'Time_Monitoring', 'Addressee_sector', 'TradablePermit', 'RegulatoryInstr', 'Energy_Other', 'Energy_LowCarbon', 'Tech_LowCarbon', 'FrameworkPolicy', 'Time_PolDuration', 'Addressee_monitored', 'Objective_QuantTarget_noCCM', 'Objective_QualIntention_noCCM', 'Resource_MonRevenues', 'Subsidies_Incentives', 'Addressee_resource', 'PublicInvt', 'Ref_PolicyAmended', 'Authority_established', 'Time_InEffect', 'TaxIncentives', 'VoluntaryAgrmt', 'RD_D', 'Objective_QuantTarget', 'Reversibility_policy', 'Time_Compliance', 'Resource_Other', 'Ref_Strategy_Agreement', 'App_LowCarbon', 'Form_sanctioning', 'Edu_Outreach', 'Authority_legislative', 'Form_monitoring', 'Addressee_default', 'App_Other', 'Objective_QualIntention'] 43\n"
     ]
    }
   ],
   "source": [
    "pol_df = pd.read_pickle(pol_dir+\"/preprocessed_dataframe.pkl\")[[\"Policy\",\"Text\",\"Tokens\",\"Curation\"]]\n",
    "all_layers= []\n",
    "all_features = []\n",
    "all_tags = []\n",
    "span_count = 0\n",
    "# how many spans for different features and layers\n",
    "for art in pol_df.index:\n",
    "    for span in pol_df.loc[art, \"Curation\"]:\n",
    "        span_count+=1\n",
    "        all_layers.append(span.layer)\n",
    "        all_features.append(span.feature)\n",
    "        all_tags.append(span.tag)\n",
    "print(Counter(all_layers))\n",
    "print(Counter(all_features))\n",
    "print(Counter(all_tags))\n",
    "print(span_count)\n",
    "print(list(set(all_features)), len(list(set(all_features))))\n",
    "print(list(set(all_tags)), len(list(set(all_tags))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c3c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_names = ['Objective', 'Compliance', 'Time', 'Resource', 'InstrumentType', 'ApplicationSpecificity', 'TechnologySpecificity', 'EnergySpecificity', 'Reference', 'Reversibility', 'Actor']\n",
    "all_feature_names = sorted(all_feature_names)\n",
    "#all_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0705b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tag_names = ['Authority_default', 'Objective_QualIntention', 'TaxIncentives', 'Unspecified', 'Addressee_default', 'Energy_LowCarbon', 'Ref_PolicyAmended', 'Ref_Strategy_Agreement', 'FrameworkPolicy', 'Reversibility_policy', 'Time_Monitoring', 'Energy_Other', 'Ref_OtherPolicy', 'Addressee_sector', 'Resource_MonSpending', 'Form_monitoring', 'Authority_legislative', 'Subsidies_Incentives', 'Tech_Other', 'Addressee_resource', 'Time_Resources', 'Authority_established', 'Objective_QualIntention_noCCM', 'RD_D', 'Resource_MonRevenues', 'Addressee_monitored', 'Time_InEffect', 'Tech_LowCarbon', 'RegulatoryInstr', 'PublicInvt', 'Objective_QuantTarget', 'Objective_QuantTarget_noCCM', 'VoluntaryAgrmt', 'TradablePermit', 'Time_PolDuration', 'Time_Compliance', 'App_Other', 'Resource_Other', 'App_LowCarbon', 'Authority_monitoring', 'Form_sanctioning', 'Edu_Outreach']\n",
    "all_tag_names = sorted(all_tag_names)\n",
    "#all_tag_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f961626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263752\n",
      "Counter({0: 210550, 1: 45356, 2: 7062, 3: 708, 4: 70, 5: 6})\n"
     ]
    }
   ],
   "source": [
    "# how many spans the tokens have\n",
    "toks_cts= []\n",
    "for art in pol_df.index:\n",
    "    for tokensp in pol_df.loc[art, \"Tokens\"]:\n",
    "        sc = tokensp.get_span_count(annotators ='Curation')\n",
    "        toks_cts.append(sc)\n",
    "print(len(toks_cts))\n",
    "print(Counter(toks_cts))\n",
    "#52298/(45356+7062+708+70+6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae31185c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " T15813 information\n",
      "CUR653 provide adequate and targeted information and advice InstrumentType\n",
      "CUR654 information and advice InstrumentType\n",
      "CUR676 information and advice Resource\n",
      "CUR677 information Resource\n",
      "\n",
      " T15815 advice\n",
      "CUR653 provide adequate and targeted information and advice InstrumentType\n",
      "CUR654 information and advice InstrumentType\n",
      "CUR676 information and advice Resource\n",
      "CUR678 advice Resource\n",
      "\n",
      " T81467 market\n",
      "CUR6566 right to switch supplier or market participants InstrumentType\n",
      "CUR6601 right to switch supplier or market participants engaged in aggregation is granted to customers in a non-discriminatory manner as regards cost, effort and time Objective\n",
      "CUR6603 market participants engaged in aggregation Actor\n",
      "CUR6604 market participants Actor\n",
      "\n",
      " T81468 participants\n",
      "CUR6566 right to switch supplier or market participants InstrumentType\n",
      "CUR6601 right to switch supplier or market participants engaged in aggregation is granted to customers in a non-discriminatory manner as regards cost, effort and time Objective\n",
      "CUR6603 market participants engaged in aggregation Actor\n",
      "CUR6604 market participants Actor\n",
      "\n",
      " T83121 customers\n",
      "CUR6848 ensure the protection of energy poor and vulnerable household customers Objective\n",
      "CUR6849 protection of energy poor and vulnerable household customers Objective\n",
      "CUR6850 energy poor and vulnerable household customers Actor\n",
      "CUR6851 customers Actor\n",
      "\n",
      " T98675 commission\n",
      "CUR8629 commission (eurostat) Actor\n",
      "CUR8630 commission (eurostat) Actor\n",
      "CUR8631 commission Actor\n",
      "CUR8632 commission Actor\n",
      "\n",
      " T98677 eurostat\n",
      "CUR8629 commission (eurostat) Actor\n",
      "CUR8630 commission (eurostat) Actor\n",
      "CUR8633 eurostat Actor\n",
      "CUR8634 eurostat Actor\n",
      "\n",
      " T99606 advice\n",
      "CUR8706 advice and information InstrumentType\n",
      "CUR8707 advice InstrumentType\n",
      "CUR8735 advice and information Resource\n",
      "CUR8736 advice Resource\n",
      "\n",
      " T99608 information\n",
      "CUR8706 advice and information InstrumentType\n",
      "CUR8708 information InstrumentType\n",
      "CUR8735 advice and information Resource\n",
      "CUR8737 information Resource\n",
      "\n",
      " T105649 labelling\n",
      "CUR9411 labelling provisions of the respective eso standards InstrumentType\n",
      "CUR9412 labelling provisions of the respective eso standards InstrumentType\n",
      "CUR9413 labelling provisions InstrumentType\n",
      "CUR9414 labelling provisions InstrumentType\n",
      "\n",
      " T105650 provisions\n",
      "CUR9411 labelling provisions of the respective eso standards InstrumentType\n",
      "CUR9412 labelling provisions of the respective eso standards InstrumentType\n",
      "CUR9413 labelling provisions InstrumentType\n",
      "CUR9414 labelling provisions InstrumentType\n",
      "\n",
      " T105654 eso\n",
      "CUR9411 labelling provisions of the respective eso standards InstrumentType\n",
      "CUR9412 labelling provisions of the respective eso standards InstrumentType\n",
      "CUR9415 eso standards InstrumentType\n",
      "CUR9441 eso Actor\n",
      "\n",
      " T105655 standards\n",
      "CUR9411 labelling provisions of the respective eso standards InstrumentType\n",
      "CUR9412 labelling provisions of the respective eso standards InstrumentType\n",
      "CUR9415 eso standards InstrumentType\n",
      "CUR9416 standards InstrumentType\n",
      "\n",
      " T119055 eu\n",
      "CUR10212 2025 and 2030 eu fleet-wide targets InstrumentType\n",
      "CUR10213 eu fleet-wide targets InstrumentType\n",
      "CUR10232 2025 and 2030 eu fleet-wide targets Objective\n",
      "CUR10233 eu fleet-wide targets Objective\n",
      "\n",
      " T119056 fleet-wide\n",
      "CUR10212 2025 and 2030 eu fleet-wide targets InstrumentType\n",
      "CUR10213 eu fleet-wide targets InstrumentType\n",
      "CUR10232 2025 and 2030 eu fleet-wide targets Objective\n",
      "CUR10233 eu fleet-wide targets Objective\n",
      "\n",
      " T119057 targets\n",
      "CUR10212 2025 and 2030 eu fleet-wide targets InstrumentType\n",
      "CUR10213 eu fleet-wide targets InstrumentType\n",
      "CUR10232 2025 and 2030 eu fleet-wide targets Objective\n",
      "CUR10233 eu fleet-wide targets Objective\n",
      "\n",
      " T123375 eu\n",
      "CUR10767 eu fleet-wide target of 147 g co2/km for the average emissions of new light commercial vehicles InstrumentType\n",
      "CUR10768 eu fleet-wide target InstrumentType\n",
      "CUR10791 eu fleet-wide target of 147 g co2/km for the average emissions of new light commercial vehicles registered in the union Objective\n",
      "CUR10792 eu fleet-wide target of 147 g co2/km for the average emissions of new light commercial vehicles Objective\n",
      "\n",
      " T123376 fleet-wide\n",
      "CUR10767 eu fleet-wide target of 147 g co2/km for the average emissions of new light commercial vehicles InstrumentType\n",
      "CUR10768 eu fleet-wide target InstrumentType\n",
      "CUR10791 eu fleet-wide target of 147 g co2/km for the average emissions of new light commercial vehicles registered in the union Objective\n",
      "CUR10792 eu fleet-wide target of 147 g co2/km for the average emissions of new light commercial vehicles Objective\n",
      "\n",
      " T123377 target\n",
      "CUR10767 eu fleet-wide target of 147 g co2/km for the average emissions of new light commercial vehicles InstrumentType\n",
      "CUR10768 eu fleet-wide target InstrumentType\n",
      "CUR10791 eu fleet-wide target of 147 g co2/km for the average emissions of new light commercial vehicles registered in the union Objective\n",
      "CUR10792 eu fleet-wide target of 147 g co2/km for the average emissions of new light commercial vehicles Objective\n",
      "\n",
      " T209083 tariffs\n",
      "CUR17410 connection tariffs InstrumentType\n",
      "CUR17411 tariffs InstrumentType\n",
      "CUR17476 connection tariffs Resource\n",
      "CUR17477 tariffs Resource\n",
      "Counter({1: 29038, 0: 21696, 2: 2328, 3: 120, 4: 20})\n"
     ]
    }
   ],
   "source": [
    "# how many spans the tokens have\n",
    "toks_cts= []\n",
    "for art in pol_df.index:\n",
    "    for tokensp in pol_df.loc[art, \"Tokens\"]:\n",
    "        sc = tokensp.get_token_spans(annotators ='Curation')\n",
    "        if sc:\n",
    "            count = 0\n",
    "            for s in sc:\n",
    "                if s.feature in [\"Actor\",\"InstrumentType\",\"Objective\",\"Resource\",\"Time\"]:\n",
    "                    count+=1\n",
    "            toks_cts.append(count)\n",
    "            if count==4:\n",
    "                print(\"\\n\",tokensp.token_id, tokensp.text)\n",
    "                for s in sc:\n",
    "                    print(s.span_id, s.text, s.feature)\n",
    "print(Counter(toks_cts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5e98e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11380\n"
     ]
    }
   ],
   "source": [
    "# for each layer and feature\n",
    "# accrue list of number of tokens\n",
    "# then calculate mean+sd, median, mode, and range\n",
    "\n",
    "layers_dct ={key:[] for key in list(dict(Counter(all_layers)))}\n",
    "features_dct ={key:[] for key in list(dict(Counter(all_features)))}\n",
    "tags_dct={key:[] for key in list(dict(Counter(all_tags)))}\n",
    "# token counts for each span of these specific layers/features\n",
    "for art in pol_df.index:\n",
    "    for span in pol_df.loc[art, \"Curation\"]:\n",
    "        layers_dct[span.layer].append(len(span.text.split(\" \")))\n",
    "        features_dct[span.feature].append(len(span.text.split(\" \")))\n",
    "        tags_dct[span.tag].append(len(span.text.split(\" \")))\n",
    "print(len(layers_dct['Policydesigncharacteristics']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cf063e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from create_datasets import get_label_set, deduped_df_fxn\n",
    "pol_a = deduped_df_fxn(pol_df, \"a\")\n",
    "len(list(pol_a.index))\n",
    "# if split(\"_\")[-1] == front or Whereas\n",
    "'''\n",
    "removal = []\n",
    "for code in list(pol_a.index):\n",
    "    if code.split(\"_\")[-1] in [\"front\", \"Whereas\"]:\n",
    "        removal.append(code)\n",
    "'''   \n",
    "removal = [code for code in list(pol_a.index) if code.split(\"_\")[-1] in [\"front\", \"Whereas\"]]     \n",
    "print(len(removal))\n",
    "pol_a = pol_a.drop(removal, axis=0)\n",
    "len(list(pol_a.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db6b1716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "mode = \"a\"\n",
    "sghead_ds = load_from_disk(cwd+f\"/inputs/{mode}/sghead_ds\")\n",
    "for row in sghead_ds:\n",
    "    if row[\"id\"].split(\"_\")[-1] in [\"front\",\"Whereas\"]:\n",
    "        print(list(set(row[\"ner_tags\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcce2236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20856\n",
      "20007\n",
      "20856\n",
      "18068\n",
      "19507\n",
      "20856\n"
     ]
    }
   ],
   "source": [
    "sp_cnt = 0\n",
    "for art in pol_df.index:\n",
    "    for span in pol_df.loc[art,\"Curation\"]:\n",
    "        sp_cnt+=1\n",
    "print(sp_cnt)\n",
    "for ltr in [\"a\",\"b\",\"c\",\"d\",\"e\"]:\n",
    "    #x = identify_dup_spans(pol_df, ltr)\n",
    "    #print(ltr, len(x))\n",
    "    new_df = deduped_df_fxn(pol_df, ltr)\n",
    "    sp_cnt = 0\n",
    "    for art in new_df.index:\n",
    "        for span in new_df.loc[art,\"Curation\"]:\n",
    "            sp_cnt+=1\n",
    "    print(sp_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d72ca49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Instrumenttypes\n",
      "Total: 8102\n",
      "Mean: 2.323487238313737 \tSD: 2.067065017042484\n",
      "Median: 2.0\n",
      "Mode: 1\n",
      "Range: 1 - 52\n",
      "\n",
      " Policydesigncharacteristics\n",
      "Total: 32441\n",
      "Mean: 2.8507029876977152 \tSD: 3.752086055706703\n",
      "Median: 2.0\n",
      "Mode: 1\n",
      "Range: 1 - 79\n",
      "\n",
      " Technologyandapplicationspecificity\n",
      "Total: 15561\n",
      "Mean: 2.5982634830522624 \tSD: 2.5907931939638527\n",
      "Median: 2.0\n",
      "Mode: 1\n",
      "Range: 1 - 56\n",
      "\n",
      " InstrumentType\n",
      "Total: 7838\n",
      "Mean: 2.3134592680047223 \tSD: 2.0796638783524948\n",
      "Median: 2.0\n",
      "Mode: 1\n",
      "Range: 1 - 52\n",
      "\n",
      " Actor\n",
      "Total: 12253\n",
      "Mean: 2.035044012622488 \tSD: 1.5666346355393967\n",
      "Median: 2.0\n",
      "Mode: 2\n",
      "Range: 1 - 27\n",
      "\n",
      " Time\n",
      "Total: 2878\n",
      "Mean: 3.606516290726817 \tSD: 3.752433815934359\n",
      "Median: 2.0\n",
      "Mode: 1\n",
      "Range: 1 - 26\n",
      "\n",
      " Compliance\n",
      "Total: 4422\n",
      "Mean: 2.417714598141061 \tSD: 3.4569021967239286\n",
      "Median: 1.0\n",
      "Mode: 1\n",
      "Range: 1 - 53\n",
      "\n",
      " Reversibility\n",
      "Total: 418\n",
      "Mean: 10.45 \tSD: 15.9952336650641\n",
      "Median: 3.0\n",
      "Mode: 1\n",
      "Range: 1 - 79\n",
      "\n",
      " Reference\n",
      "Total: 3326\n",
      "Mean: 2.8162574089754444 \tSD: 2.6355107472792207\n",
      "Median: 2.0\n",
      "Mode: 2\n",
      "Range: 1 - 37\n",
      "\n",
      " Objective\n",
      "Total: 7852\n",
      "Mean: 7.915322580645161 \tSD: 7.4814883842941855\n",
      "Median: 5.0\n",
      "Mode: 3\n",
      "Range: 1 - 49\n",
      "\n",
      " ApplicationSpecificity\n",
      "Total: 4345\n",
      "Mean: 2.803225806451613 \tSD: 2.8972716722047025\n",
      "Median: 2.0\n",
      "Mode: 1\n",
      "Range: 1 - 38\n",
      "\n",
      " EnergySpecificity\n",
      "Total: 4195\n",
      "Mean: 2.198637316561845 \tSD: 2.0562983545604614\n",
      "Median: 2.0\n",
      "Mode: 1\n",
      "Range: 1 - 36\n",
      "\n",
      " TechnologySpecificity\n",
      "Total: 7014\n",
      "Mean: 2.776722090261283 \tSD: 2.718086343825203\n",
      "Median: 2.0\n",
      "Mode: 2\n",
      "Range: 1 - 56\n",
      "\n",
      " InstrumentType_2\n",
      "Total: 261\n",
      "Mean: 2.663265306122449 \tSD: 1.5448656077274787\n",
      "Median: 2.0\n",
      "Mode: 2\n",
      "Range: 1 - 10\n",
      "\n",
      " Resource\n",
      "Total: 1250\n",
      "Mean: 2.4461839530332683 \tSD: 3.298066224227501\n",
      "Median: 2.0\n",
      "Mode: 1\n",
      "Range: 1 - 48\n",
      "\n",
      " end\n",
      "Total: 52\n",
      "Mean: 3.7142857142857144 \tSD: 3.5340905362437094\n",
      "Median: 2.5\n",
      "Mode: 1\n",
      "Range: 1 - 14\n"
     ]
    }
   ],
   "source": [
    "for layer in list(layers_dct):\n",
    "    print(\"\\n\", layer)\n",
    "    values = layers_dct[layer]\n",
    "    print(\"Total:\", np.sum(values))\n",
    "    print(\"Mean:\", np.mean(values), \"\\tSD:\", np.std(values))\n",
    "    print(\"Median:\", np.median(values))\n",
    "    bc = np.bincount(values)\n",
    "    print(\"Mode:\", np.argmax(bc))\n",
    "    print(\"Range:\", np.min(values), \"-\", np.max(values))\n",
    "\n",
    "for feature in list(features_dct):\n",
    "    print(\"\\n\", feature)\n",
    "    values = features_dct[feature]\n",
    "    print(\"Total:\", np.sum(values))\n",
    "    print(\"Mean:\", np.mean(values), \"\\tSD:\", np.std(values))\n",
    "    print(\"Median:\", np.median(values))\n",
    "    bc = np.bincount(values)\n",
    "    print(\"Mode:\", np.argmax(bc))\n",
    "    print(\"Range:\", np.min(values), \"-\", np.max(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56618e3d",
   "metadata": {},
   "source": [
    "## Validating our datasets and dataset dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09f9bea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "Counter({'O': 232246, 'I-Actor': 6668, 'I-Objective': 6626, 'B-Actor': 5807, 'I-InstrumentType': 3904, 'B-InstrumentType': 3090, 'I-Time': 2467, 'B-Objective': 952, 'B-Time': 781, 'I-Resource': 710, 'B-Resource': 501})\n",
      "b\n",
      "Counter({'O': 210550, 'I-Policydesigncharacteristics': 22731, 'B-Policydesigncharacteristics': 10781, 'I-Technologyandapplicationspecificity': 7767, 'B-Technologyandapplicationspecificity': 5548, 'I-Instrumenttypes': 3515, 'B-Instrumenttypes': 2860})\n",
      "c\n",
      "Counter({'O': 211241, 'I-Actor': 6204, 'B-Actor': 5641, 'I-Objective': 5445, 'I-Reference': 5322, 'I-TechnologySpecificity': 3406, 'I-InstrumentType': 3384, 'B-InstrumentType': 2779, 'I-Time': 2449, 'I-Compliance': 2282, 'B-TechnologySpecificity': 2258, 'I-ApplicationSpecificity': 2182, 'I-EnergySpecificity': 2177, 'B-EnergySpecificity': 1881, 'B-Compliance': 1767, 'B-ApplicationSpecificity': 1405, 'B-Reference': 1166, 'B-Objective': 894, 'B-Time': 780, 'I-Resource': 604, 'B-Resource': 485})\n",
      "d\n",
      "Counter({'O': 236344, 'I-Objective': 6742, 'I-Actor': 6669, 'B-Actor': 5808, 'I-InstrumentType': 4045, 'B-InstrumentType': 3186, 'B-Objective': 958})\n"
     ]
    }
   ],
   "source": [
    "for mode in [\"a\", \"b\", \"c\", \"d\"]:\n",
    "    print(mode)\n",
    "    ds = Dataset.load_from_disk(f\"{cwd}/inputs/{mode}/sghead_ds\")\n",
    "    flatList = [element for innerList in ds['ner_tags'] for element in innerList]\n",
    "    print(Counter(flatList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in [\"a\", \"b\", \"c\", \"d\"]:\n",
    "    print(mode)\n",
    "    ds = Dataset.load_from_disk(f\"{cwd}/inputs/{mode}/mhead_ds\")\n",
    "    for cat in list(ds.features):\n",
    "        if cat not in ['id', 'text', 'tokens']:\n",
    "            flatList = [element for innerList in ds[cat] for element in innerList]\n",
    "            print(Counter(flatList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeaba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0\n",
    "for mode in [\"a\", \"b\", \"c\", \"d\"]:\n",
    "    dsdct = DatasetDict.load_from_disk(f\"{cwd}/inputs/{mode}/sghead_dsdcts/dsdct_r{r}\")\n",
    "    print(dsdct)\n",
    "    dsdct = DatasetDict.load_from_disk(f\"{cwd}/inputs/{mode}/mhead_dsdcts/dsdct_r{r}\")\n",
    "    print(dsdct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4614be43",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e263d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import numpy as np\n",
    "sys.path.insert(0, '..')\n",
    "from collections import Counter\n",
    "from transformers import pipeline, AutoModel, PreTrainedTokenizerBase, AutoConfig, PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import evaluate\n",
    "from typing import Any, Dict, List\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "cwd = os.getcwd()\n",
    "pol_dir = cwd+\"/../src/d01_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a3407b",
   "metadata": {},
   "source": [
    "so many things to try even just for NER, especially given how long some of these token sequences are\n",
    "\n",
    "trying multiple classifiction heads\n",
    "- different base models\n",
    "- using last hidden state vs using last few hidden states (average or concatenation)\n",
    "- weighted vs unweighted loss\n",
    "- evaluation metrics (token micro F1 or overlap instead of seqeval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a896dc",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aaab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghead_tokenize_and_align_labels(tokens, ner_tags, tokenizer, label2id, max_length=512):\n",
    "    tokenized_inputs = tokenizer(tokens, truncation=True, is_split_into_words=True, max_length=max_length, return_tensors=None)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(label2id[ner_tags[word_idx]])\n",
    "        else:\n",
    "            label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "class SgheadDataset(TorchDataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_length=512):\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_list = ['O', 'B-Actor', 'I-Actor', 'B-InstrumentType', 'I-InstrumentType', 'B-Objective', 'I-Objective', 'B-Resource', 'I-Resource', 'B-Time', 'I-Time']\n",
    "        self.label2id = {l: i for i, l in enumerate(self.label_list)}\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        tokens = sample[\"tokens\"]\n",
    "        ner_tags = sample[\"ner_tags\"]\n",
    "\n",
    "        encoding = sghead_tokenize_and_align_labels(\n",
    "            tokens,\n",
    "            ner_tags,\n",
    "            self.tokenizer,\n",
    "            self.label2id,\n",
    "            self.max_length\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(encoding[\"input_ids\"]),\n",
    "            \"attention_mask\": torch.tensor(encoding[\"attention_mask\"]),\n",
    "            \"labels\": torch.tensor(encoding[\"labels\"])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ccf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def sghead_collate_fn(batch, pad_token_id):\n",
    "    input_ids = [b[\"input_ids\"] for b in batch]\n",
    "    attention_masks = [b[\"attention_mask\"] for b in batch]\n",
    "    labels = [b[\"labels\"] for b in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=pad_token_id)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be3b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "r=0\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dataset_dict = DatasetDict.load_from_disk(cwd+f\"/inputs/sghead_dsdcts/dsdct_r{r}\")\n",
    "\n",
    "train_dataset = SgheadDataset(\n",
    "    dataset_dict[\"train\"],\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "dev_dataset = SgheadDataset(\n",
    "    dataset_dict[\"dev\"],\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda b: sghead_collate_fn(b, tokenizer.pad_token_id)\n",
    ")\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: sghead_collate_fn(b, tokenizer.pad_token_id)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407b3384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, get_linear_schedule_with_warmup\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "label_list = ['O', 'B-Actor', 'I-Actor', 'B-InstrumentType', 'I-InstrumentType', 'B-Objective', 'I-Objective', 'B-Resource', 'I-Resource', 'B-Time', 'I-Time']\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for i, l in enumerate(label_list)}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=(model_name == model_name)\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} | Train loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c14657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            labels = batch[\"labels\"]\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            labels = labels.numpy()\n",
    "\n",
    "            for preds, labs in zip(predictions, labels):\n",
    "                true_preds = []\n",
    "                true_labs = []\n",
    "                for p, l in zip(preds, labs):\n",
    "                    if l != -100:\n",
    "                        true_preds.append(id2label[p])\n",
    "                        true_labs.append(id2label[l])\n",
    "                all_preds.append(true_preds)\n",
    "                all_labels.append(true_labs)\n",
    "\n",
    "    return seqeval.compute(predictions=all_preds, references=all_labels)\n",
    "\n",
    "metrics = evaluate_model(model, dev_loader)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7229de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"{model_save_addr}/{model_name.split('/')[-1]}_{r}\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e12249",
   "metadata": {},
   "source": [
    "## Messin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800da6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SgheadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx: int):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_addr = cwd+\"/inputs/sghead_ds\"\n",
    "ds.save_to_disk(dir_addr)\n",
    "print(f\"Created dataset in {dir_addr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197dbdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_to_mhead_lbls(feature_name, tokens, spans):\n",
    "    '''\n",
    "    Helper function to df_to_mhead_dataset(df)\n",
    "    For an article in the dataframe, for a specific feature type in the annotated spans,\n",
    "    creates a list of token labels in bio format then converts to integers.\n",
    "    \n",
    "    :param feature_name: Name of feature whose BIO list is being created for the article\n",
    "    :param tokens: list of token objects\n",
    "    :param spans: list of span objects\n",
    "    :param label2id: dictionary mapping labels to integers\n",
    "    '''\n",
    "    token_labels = [\"O\"] * len(tokens)\n",
    "    for spn in spans:\n",
    "        if spn.feature == feature_name:\n",
    "            start_char = spn.start\n",
    "            end_char = spn.stop\n",
    "            inside_tokens = []\n",
    "            for i, tok in enumerate(tokens):\n",
    "                tok_start = tok.start\n",
    "                tok_end = tok.stop\n",
    "                overlap = not (tok_end <= start_char or tok_start >= end_char)\n",
    "                if overlap:\n",
    "                    inside_tokens.append(i)\n",
    "            if inside_tokens:\n",
    "                token_labels[inside_tokens[0]] = f\"B\"\n",
    "                for i in inside_tokens[1:]:\n",
    "                    token_labels[i] = f\"I\"\n",
    "    return token_labels\n",
    "\n",
    "def df_to_mhead_ds(df):\n",
    "    '''\n",
    "    Converts pandas dataframe to huggingface dataset\n",
    "    ***Currently ignores any articles longer than 512 tokens\n",
    "    \n",
    "    :param df: POLIANNA dataframe\n",
    "    '''\n",
    "    datapoints = []\n",
    "    for artid in df.index:\n",
    "        tokens = df.loc[artid,\"Tokens\"]\n",
    "        if len(tokens) <= 512: # we'll change this eventually\n",
    "            text = df.loc[artid,\"Text\"]\n",
    "            spans = df.loc[artid,\"Curation\"]\n",
    "            token_texts = [t.text for t in tokens]\n",
    "            datapoint = {}\n",
    "            datapoint['id'] = artid\n",
    "            datapoint[\"text\"] = text\n",
    "            datapoint[\"tokens\"] = token_texts\n",
    "            for ftr in [\"Actor\", \"InstrumentType\", \"Objective\", \"Resource\", \"Time\"]:\n",
    "                token_level_labels = span_to_mhead_lbls(ftr, tokens, spans)\n",
    "                datapoint[f\"labels_{ftr}\"] = token_level_labels\n",
    "            datapoints.append(datapoint)\n",
    "    return Dataset.from_list(datapoints)\n",
    "\n",
    "dir_addr = cwd+\"/inputs/mhead_ds\"\n",
    "ds = df_to_mhead_ds(pol_df)\n",
    "ds.save_to_disk(dir_addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3104e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in zip(ds['tokens'][0], ds['ner_tags'][0]):\n",
    "    print(ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e23a535",
   "metadata": {},
   "source": [
    "https://medium.com/@shahrukhx01/multi-task-learning-with-transformers-part-1-multi-prediction-heads-b7001cf014bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d51d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_to_bio_tok_lbls(feature_name, tokens, spans, label2id):\n",
    "    token_labels = [\"O\"] * len(tokens)\n",
    "    for spn in spans:\n",
    "        if spn.feature == feature_name:\n",
    "            start_char = spn.start\n",
    "            end_char = spn.stop\n",
    "            inside_tokens = []\n",
    "            for i, tok in enumerate(tokens):\n",
    "                tok_start = tok.start\n",
    "                tok_end = tok.stop\n",
    "                overlap = not (tok_end <= start_char or tok_start >= end_char)\n",
    "                if overlap:\n",
    "                    inside_tokens.append(i)\n",
    "            if inside_tokens:\n",
    "                token_labels[inside_tokens[0]] = f\"B\"\n",
    "                for i in inside_tokens[1:]:\n",
    "                    token_labels[i] = f\"I\"\n",
    "    return [label2id[l] for l in token_labels]\n",
    "\n",
    "def df_to_dataset(df):\n",
    "    label2id = {\n",
    "        \"O\":0, \"B\":1, \"I\":2\n",
    "    }\n",
    "    dataset = {\n",
    "        \"id\":[],\n",
    "        \"text\":[],\n",
    "        \"tokens\":[],\n",
    "        \"labels_Actor\":[],\n",
    "        \"labels_InstrumentType\":[],\n",
    "        \"labels_Objective\":[],\n",
    "        \"labels_Resource\":[],\n",
    "        \"labels_Time\":[]\n",
    "    }\n",
    "    for artid in df.index:\n",
    "        tokens = df.loc[artid,\"Tokens\"]\n",
    "        if len(tokens) <= 512: # we'll change this eventually\n",
    "            text = df.loc[artid,\"Text\"]\n",
    "            spans = df.loc[artid,\"Curation\"]\n",
    "            token_texts = [t.text for t in tokens]\n",
    "            dataset['id'].append(artid)\n",
    "            dataset[\"text\"].append(text)\n",
    "            dataset[\"tokens\"].append(token_texts)\n",
    "            for ftr in [\"Actor\", \"InstrumentType\", \"Objective\", \"Resource\", \"Time\"]:\n",
    "                token_level_labels = span_to_bio_tok_lbls(ftr, tokens, spans, label2id)\n",
    "                dataset[f\"labels_{ftr}\"].append(token_level_labels)\n",
    "    return Dataset.from_dict(dataset), list(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a881ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, label_list = df_to_dataset(pol_df)\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "for i, lbl in enumerate(label_list):\n",
    "    id2label[i] = lbl\n",
    "    label2id[lbl] = i\n",
    "# do the datasets need to differ by model used for tokenization of results too??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for r in [0,1,2]:\n",
    "    td_test = dataset.train_test_split(test_size=0.2, seed=r)\n",
    "    train_dev = td_test['train'].train_test_split(test_size=0.25, seed=r)\n",
    "    ds_dct = DatasetDict({\"train\":train_dev['train'], \"dev\":train_dev['test'], \"test\":td_test['test']})\n",
    "    print(ds_dct)\n",
    "    ds_dct.save_to_disk(cwd+f\"/inputs/sep/dsdct_r{r}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6fc263",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2019d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "        \"O\":0, \"B\":1, \"I\":2\n",
    "    }\n",
    "id2label = {\n",
    "    0:\"O\", 1:\"B\", 2:\"I\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/deberta-v3-base\" # suggested lr of 3e-5\n",
    "#model_name = \"dslim/bert-base-NER-uncased\"\n",
    "#model_name = \"FacebookAI/xlm-roberta-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504416f8",
   "metadata": {},
   "source": [
    "have to adapt the tokenizing and aligning script to account for the separate label lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eef125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all feature/label types\n",
    "label_cols = [\n",
    "    \"labels_Actor\",\n",
    "    \"labels_InstrumentType\",\n",
    "    \"labels_Objective\",\n",
    "    \"labels_Resource\",\n",
    "    \"labels_Time\"\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # adapted for multi-head from https://huggingface.co/docs/transformers/en/tasks/token_classification\n",
    "    # even tho the token lists area already split into words, we need to break them into subwords\n",
    "    # and then ensure that the label sequences still align in the new token sequence\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True, return_attention_mask=True)\n",
    "    # for each label type/list\n",
    "    for col in label_cols:\n",
    "        all_aligned_labels = []\n",
    "        # loop through this label type's sequence in each sample and realign\n",
    "        for sample_idx, labels in enumerate(examples[col]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=sample_idx)\n",
    "            # smth like [None, 0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 19, 20]\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(labels[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "                previous_word_idx = word_idx\n",
    "            all_aligned_labels.append(label_ids)\n",
    "        tokenized_inputs[col] = all_aligned_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe1f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=0\n",
    "dataset_dict = DatasetDict.load_from_disk(cwd+f\"/inputs/sep/dsdct_r{r}\")\n",
    "tokenized_dsdct = dataset_dict.map(tokenize_and_align_labels, batched=True)\n",
    "#tokenized_dsdct.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"] + label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checkign\n",
    "tokenized_inputs = tokenizer(dataset_dict['train'][0:5][\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "for sample_idx, labels in enumerate(dataset_dict['train'][0:5]['labels_Actor']):\n",
    "    word_ids = tokenized_inputs.word_ids(batch_index=sample_idx)\n",
    "    print(word_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec6640",
   "metadata": {},
   "source": [
    "new custom data collator for multi-heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50108d88",
   "metadata": {},
   "source": [
    "we need a new data collator because we have mutliple label lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadDataCollator:\n",
    "    '''\n",
    "    Using PreTrainedTokenizerBase i.e. whatever pretrained tokenizer we have from tokenize_and_align_labels\n",
    "    And using pad_sequence\n",
    "    '''\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase, label_columns: List[str], padding=True, max_length=None):\n",
    "        #initializing the essentials\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_columns = label_columns\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "    def __call__(self, features):\n",
    "        input_ids = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in features]\n",
    "        #attention_mask = [torch.tensor(f[\"attention_mask\"], dtype=torch.long) for f in features] # something isnt working\n",
    "        attention_mask = [\n",
    "            torch.tensor(f.get(\"attention_mask\", [1]*len(f[\"input_ids\"])), dtype=torch.long)\n",
    "            for f in features\n",
    "        ]\n",
    "        # padding inputids and attnmask\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "        # batch\n",
    "        batch = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "        # padding labels\n",
    "        for col in self.label_columns:\n",
    "            #label_lists = [torch.tensor(f[col], dtype=torch.long) for f in features] # something isnt working here either\n",
    "            label_lists = [\n",
    "                torch.tensor(f.get(col, [-100]*len(f[\"input_ids\"])), dtype=torch.long)\n",
    "                for f in features\n",
    "            ]\n",
    "            labels_padded = pad_sequence(label_lists, batch_first=True, padding_value=-100)\n",
    "            # then finally adding to batch\n",
    "            batch[col] = labels_padded\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e5800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checking\n",
    "data_collator = MultiHeadDataCollator(tokenizer=tokenizer, label_columns=label_cols, max_length=512)\n",
    "sample_batch = [tokenized_dsdct['train'][i] for i in range(16)]\n",
    "batch = data_collator(sample_batch)\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c4920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity checking\n",
    "tokens = tokenizer.convert_ids_to_tokens(batch[\"input_ids\"][0])\n",
    "for idx, tok in enumerate(tokens[:50]):\n",
    "    print(f\"{idx:03} | {tok:15} | \"\n",
    "          f\"A:{batch['labels_Actor'][0][idx].item():2}  \"\n",
    "          f\"T:{batch['labels_Time'][0][idx].item():2}  \"\n",
    "          f\"I:{batch['labels_InstrumentType'][0][idx].item():2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139ecff",
   "metadata": {},
   "source": [
    "new model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35224f75",
   "metadata": {},
   "source": [
    "microsoft/deberta-v3-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3782cb2d",
   "metadata": {},
   "source": [
    "#### Weight calculations (currently only using train split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(label_cols, tokenized_dsdct):\n",
    "    # lets create class (BIO) weights for each feature type\n",
    "    num_classes = 3\n",
    "    class_weights = {}\n",
    "    class_weights_norm = {}\n",
    "    for lname in label_cols:\n",
    "        name = lname.replace(\"labels_\", \"\")\n",
    "        labels = np.concatenate([np.array(l) for l in tokenized_dsdct['train'][lname]])\n",
    "        labels = labels[labels != -100]  # remove padding\n",
    "        counter = Counter(labels)\n",
    "        # inverse frequency weighting\n",
    "        weights=[0]*num_classes\n",
    "        for i in range(num_classes):\n",
    "            count = counter.get(i, 0)\n",
    "            if count == 0: #handle no-shows so no zer-os (no dividing by zeros that is)\n",
    "                weights[i] = 1.0\n",
    "            else:\n",
    "                weights[i] = len(labels) / (num_classes * count)\n",
    "        class_weights[name] = torch.tensor(weights, dtype=torch.float)\n",
    "        # old vers\n",
    "        #total = sum(counter.values())\n",
    "        #class_weights[name] = torch.tensor([total/(num_classes*counter[i]) for i in range(num_classes)], dtype=torch.float)\n",
    "        # then normalize\n",
    "        w = torch.tensor(weights, dtype=torch.float)\n",
    "        w = w / w.mean()\n",
    "        class_weights_norm[name] = w\n",
    "    #cls_wt_tot = sum([class_weights[cls] for cls in list(class_weights)])\n",
    "    #class_weights_norm = {cls: torch.tensor(class_weights[cls]/cls_wt_tot, dtype=torch.float) for cls in list(class_weights)}\n",
    "    # weights for each head\n",
    "    head_counts = {}\n",
    "    for lname in label_cols:\n",
    "        name = lname.replace(\"labels_\", \"\")\n",
    "        # concatenate all labels and remove -100s\n",
    "        labels = np.concatenate([np.array(l) for l in tokenized_dsdct['train'][lname]])\n",
    "        labels = labels[labels != -100]\n",
    "        labels = labels[labels != 0]\n",
    "        head_counts[name] = len(labels) # only tokens B or I\n",
    "    total_tokens = sum(head_counts.values())\n",
    "    head_weights = {head: total_tokens / (len(head_counts) * count) for head, count in head_counts.items()}\n",
    "    #hd_wt_tot = sum([head_weights[cls] for cls in list(head_weights)])\n",
    "    #head_weights_norm = {head: math.log(head_weights[head]) for head in list(head_weights)}\n",
    "    w = torch.tensor(list(head_weights.values()), dtype=torch.float)\n",
    "    w = w / w.mean()\n",
    "    head_weights_norm = {head: w[i] for i, head in enumerate(head_weights.keys())}\n",
    "    return {\"class_weights\": class_weights, \"class_weights_norm\": class_weights_norm, \"head_weights\": head_weights, \"head_weights_norm\": head_weights_norm}\n",
    "\n",
    "WEIGHTS = get_weights(label_cols, tokenized_dsdct)\n",
    "WEIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768c37d",
   "metadata": {},
   "source": [
    "##### spoiler\n",
    "weights = {'class_weights': {'Actor': torch.tensor([ 0.3560,  9.8114, 11.2143], dtype=torch.float),\n",
    "  'InstrumentType': torch.tensor([ 0.3483, 16.4053, 14.7887], dtype=torch.float),\n",
    "  'Objective': torch.tensor([ 0.3513, 53.5521,  7.4079], dtype=torch.float),\n",
    "  'Resource': torch.tensor([ 0.3367, 92.7964, 53.0844], dtype=torch.float),\n",
    "  'Time': torch.tensor([ 0.3412, 59.8834, 19.0240], dtype=torch.float)},\n",
    " 'class_weights_norm': {'Actor': torch.tensor([-1.0328,  2.2835,  2.4172], dtype=torch.float),\n",
    "  'InstrumentType': torch.tensor([-1.0548,  2.7976,  2.6939], dtype=torch.float),\n",
    "  'Objective': torch.tensor([-1.0460,  3.9807,  2.0025], dtype=torch.float),\n",
    "  'Resource': torch.tensor([-1.0887,  4.5304,  3.9719], dtype=torch.float),\n",
    "  'Time': torch.tensor([-1.0753,  4.0924,  2.9457], dtype=torch.float)},\n",
    " 'head_weights': {'Actor': 0.5988807576409815,\n",
    "  'InstrumentType': 0.8900831733845169,\n",
    "  'Objective': 0.7447537473233404,\n",
    "  'Resource': 3.8644444444444446,\n",
    "  'Time': 1.6522565320665084},\n",
    " 'head_weights_norm': {'Actor': -0.5126927697303358,\n",
    "  'InstrumentType': -0.11644036738140337,\n",
    "  'Objective': -0.2947016557487147,\n",
    "  'Resource': 1.3518179315899177,\n",
    "  'Time': 0.5021419487977465}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ab61a",
   "metadata": {},
   "source": [
    "since loss is computed per token instead of per span, we'll look at the All instead of the Ents results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e41bad8",
   "metadata": {},
   "source": [
    "## Training components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56fc5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadTokenConfig(PretrainedConfig):\n",
    "    model_type = \"deberta-multihead\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            base_model_name=\"microsoft/deberta-v3-base\",\n",
    "            n_labels=3,\n",
    "            heads=None,\n",
    "            hidden_size=None,\n",
    "            id2label=None,\n",
    "            label2id=None,\n",
    "            **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_model_name = base_model_name\n",
    "        self.n_labels = n_labels\n",
    "        self.heads = heads or [\"Actor\", \"InstrumentType\", \"Objective\", \"Resource\", \"Time\"]\n",
    "        self.hidden_size = hidden_size \n",
    "        self.id2label = id2label or {0: \"O\", 1: \"B\", 2: \"I\"}\n",
    "        self.label2id = label2id or {v: k for k, v in self.id2label.items()}\n",
    "\n",
    "class DebertaForMultiHeadTokClass(PreTrainedModel):\n",
    "    config_class = MultiHeadTokenConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.encoder_config = AutoConfig.from_pretrained(config.base_model_name)\n",
    "        self.encoder = AutoModel.from_pretrained(config.base_model_name, config=self.encoder_config)\n",
    "        hidden_size = self.base_model.config.hidden_size\n",
    "        #sep linear head for each feature type classification\n",
    "        self.classifiers = nn.ModuleDict({\n",
    "            head: nn.Linear(hidden_size, config.num_labels) for head in config.heads\n",
    "        })\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob if hasattr(config, 'hidden_dropout_prob') else 0.1)\n",
    "        self.init_weights()\n",
    "    def forward(self, input_ids, attention_mask=None, **labels):\n",
    "        # batch of inputs encoded by base model\n",
    "        outputs = self.encoder(input_ids, attention_mask=attention_mask)\n",
    "        # only uses last hidden state... for now\n",
    "        # will look into averaging/concatenating last few hidden states\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        #sequence_output = self.dropout(sequence_output)\n",
    "        # passes encoded input sequence to each classifier to get logits\n",
    "        logits = {name: self.classifiers[name](sequence_output) for name in self.classifiers}\n",
    "        loss = None\n",
    "        if labels:\n",
    "            loss = 0\n",
    "            # for labels_Feature, tensor(batch_sz,seq_ln)\n",
    "            for lname, label in labels.items():\n",
    "                if label is not None:\n",
    "                    name = lname.replace(\"labels_\", \"\")\n",
    "                    # flatten attn mask\n",
    "                    active_loss = attention_mask.view(-1) == 1\n",
    "                    # get active logits, flatten to (num_act_tokens, num_classes) \n",
    "                    # then apply active loss mask (to both logits and labels)\n",
    "                    active_logits = logits[name].view(-1, 3)[active_loss]\n",
    "                    active_labels = label.view(-1)[active_loss]\n",
    "                    # weighting BIO classes for this feature\n",
    "                    #weight = WEIGHTS['class_weights'][name].to(active_logits.device)\n",
    "                    weight = WEIGHTS['class_weights_norm'][name].to(active_logits.device)\n",
    "                    loss_fct = nn.CrossEntropyLoss(weight=weight)\n",
    "                    # computing loss for this head\n",
    "                    head_loss = loss_fct(active_logits, active_labels)\n",
    "                    # weight the loss for this head\n",
    "                    head_loss *= WEIGHTS['head_weights_norm'][name]\n",
    "                    # sum loss across heads for single update to train simultaneously\n",
    "                    loss += head_loss\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on model that concatenates or averages last few hidden states for encoded representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9252146b",
   "metadata": {},
   "source": [
    "new compute_metrics for token micro-f1 instead of seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded9e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_multihead(p):\n",
    "    prediction_dct, label_dct = p\n",
    "    lblnames = [i[0] for i in prediction_dct.items()]\n",
    "    metrics = {}\n",
    "    # for each head\n",
    "    for head_name, logits in prediction_dct.items():\n",
    "        labels = label_dct[lblnames.index(head_name)] # size (batch, seq_len)\n",
    "        labels_flat = labels.flatten()\n",
    "        preds_flat = np.argmax(logits, axis=-1).flatten()\n",
    "        # mask out -100s\n",
    "        mask = labels_flat != -100\n",
    "        labels_flat = labels_flat[mask]\n",
    "        preds_flat = preds_flat[mask]\n",
    "        # micro F1\n",
    "        f1 = f1_score(labels_flat, preds_flat, average='micro')\n",
    "        metrics[f\"{head_name}_f1\"] = f1\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d40558",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = MultiHeadDataCollator(tokenizer=tokenizer, label_columns=label_cols, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83adb972",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # Extract labels for each head from the inputs\n",
    "        labels = {k: inputs.pop(k) for k in list(inputs.keys()) if k.startswith(\"labels_\")}\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs, **labels)\n",
    "        # Your model returns TokenClassifierOutput\n",
    "        loss = outputs.loss\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979f2dfd",
   "metadata": {},
   "source": [
    "## Where thamagic happpens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74648ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name.split(\"/\")[-1],\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    label_names=label_cols\n",
    ")\n",
    "\n",
    "#trainer = Trainer(\n",
    "trainer = MultiHeadTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dsdct[\"train\"],\n",
    "    eval_dataset=tokenized_dsdct[\"dev\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics_multihead\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(cwd+f\"/models/sep/{model_name.split('/')[-1]}_{r}\")\n",
    "del model\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ede2b4",
   "metadata": {},
   "source": [
    "add early stopping to trainer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3387f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0e0116",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8767113a",
   "metadata": {},
   "source": [
    "old metrics (seqeval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c83f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "mode = \"sep\"\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "id2label={0: \"O\", 1: \"B\", 2: \"I\"}\n",
    "label2id={\"O\":0, \"B\":1, \"I\":2}\n",
    "results_dict = {\n",
    "    \"microsoft/deberta-v3-base\":{},\n",
    "    \"FacebookAI/xlm-roberta-base\":{},\n",
    "    \"dslim/bert-base-NER-uncased\":{}\n",
    "}\n",
    "for model_name in list(results_dict):\n",
    "    results_dict[model_name][\"Overall\"] = {\"precision\":[], \"recall\":[], \"f1\":[], \"accuracy\":[]}\n",
    "    for ftr in [\"Actor\", \"InstrumentType\", \"Objective\", \"Resource\", \"Time\"]:\n",
    "        results_dict[model_name][ftr] = {\"precision\":[], \"recall\":[], \"f1\":[], \"number\":[]}\n",
    "for model_name in results_dict:\n",
    "    for r in [0, 1, 2]:\n",
    "        dataset_dict = DatasetDict.load_from_disk(cwd + f\"/inputs/{mode}/dsdct_r{r}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Load your multi-head model\n",
    "        config = MultiHeadTokenConfig(\n",
    "            base_model_name=model_name,   # e.g., \"microsoft/deberta-v3-base\"\n",
    "            num_labels=3,                 # per head\n",
    "            heads=[\"Actor\", \"InstrumentType\", \"Objective\", \"Resource\", \"Time\"],\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            hidden_size=768                # or get from base model config\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        model_tt = DebertaForMultiHeadTokClass.from_pretrained(cwd+f\"/models/{mode}/{model_name.split('/')[-1]}_{r}\")\n",
    "\n",
    "        model_tt.to(\"cuda\")\n",
    "        model_tt.eval()\n",
    "\n",
    "        texts = list(dataset_dict['test']['text'])\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "        # Get logits per head\n",
    "        with torch.no_grad():\n",
    "            model_inputs = {\n",
    "                \"input_ids\": inputs[\"input_ids\"],\n",
    "                \"attention_mask\": inputs[\"attention_mask\"]\n",
    "            }\n",
    "            outputs = model_tt(**model_inputs)\n",
    "            #outputs = model_tt(**inputs)\n",
    "            logits_dict  = outputs.logits  # dict of (batch, seq_len, n_classes)\n",
    "        label_list = [id2label[i] for i in range(len(id2label))]\n",
    "        head_label_lists = {\n",
    "            head: [config.id2label[i] for i in range(len(config.id2label))]\n",
    "            for head in config.heads\n",
    "        }\n",
    "\n",
    "        for head_name, logit_tensor in logits_dict.items():\n",
    "            preds = torch.argmax(logit_tensor, dim=-1).cpu().numpy()\n",
    "\n",
    "            labels = [dataset_dict['test'][f\"labels_{head_name}\"][i] for i in range(len(dataset_dict['test']))]\n",
    "\n",
    "            true_predictions = []\n",
    "            true_labels = []\n",
    "\n",
    "            for pred_seq, label_seq in zip(preds, labels):\n",
    "                pred_labels = []\n",
    "                gold_labels = []\n",
    "                for p, l in zip(pred_seq, label_seq):\n",
    "                    if l != -100:  # ignore padding\n",
    "                        pred_labels.append(head_label_lists[head_name][p])\n",
    "                        gold_labels.append(head_label_lists[head_name][l])\n",
    "                true_predictions.append(pred_labels)\n",
    "                true_labels.append(gold_labels)\n",
    "\n",
    "            results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "            print(f\"Head: {head_name}\", results)\n",
    "\n",
    "            for k in list(results):\n",
    "                if k[:4]==\"over\":\n",
    "                    x, metric = k.split(\"_\")\n",
    "                    results_dict[model_name]['Overall'][metric].append(float(results[k]))\n",
    "                else:\n",
    "                    for mtr in list(results[k]):\n",
    "                        results_dict[model_name][head_name][mtr].append(float(results[k][mtr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in list(results_dict):\n",
    "    print(f\"\\n{m}\")\n",
    "    for res in list(results_dict[m]):\n",
    "        print(f\"{res}\")\n",
    "        print(results_dict[m][res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062d1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"2nd_results_separateheads_seqeval\"\n",
    "with open(cwd+f\"/outputs/{fn}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1569a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in list(results_dict):\n",
    "    print(f\"\\n{m}\")\n",
    "    for res in list(results_dict[m]):\n",
    "        print(f\"\\n{res}\")\n",
    "        df = pd.DataFrame(results_dict[m][res])\n",
    "        df.loc['mean'] = df.mean()\n",
    "        print(round(df.loc['mean']*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a815e5e",
   "metadata": {},
   "source": [
    "new metrics -- token micro f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b281233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_multihead_res(p):\n",
    "    prediction_dct, label_dct = p\n",
    "    lblnames = [i[0] for i in prediction_dct.items()]\n",
    "    metrics = {head.replace('label_',\"\"): {} for head in lblnames}\n",
    "    # for each head\n",
    "    for head_name, logits in prediction_dct.items():\n",
    "        labels = label_dct[\"labels_\"+head_name] # size (batch, seq_len)\n",
    "        preds_flat = logits.argmax(dim=-1).flatten().cpu().numpy()\n",
    "        labels_flat = labels.flatten().cpu().numpy()\n",
    "        # mask out -100s\n",
    "        mask = labels_flat != -100\n",
    "        labels_flat = labels_flat[mask]\n",
    "        preds_flat = preds_flat[mask]\n",
    "        # micro F1\n",
    "        f1 = f1_score(labels_flat, preds_flat, average='micro')\n",
    "        metrics[head_name][\"f1\"] = f1\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"sep\"\n",
    "results_dict = {\n",
    "    \"microsoft/deberta-v3-base\":{},\n",
    "    \"FacebookAI/xlm-roberta-base\":{},\n",
    "    \"dslim/bert-base-NER-uncased\":{}\n",
    "}\n",
    "for model_name in list(results_dict):\n",
    "    #results_dict[model_name][\"Overall\"] = {\"precision\":[], \"recall\":[], \"f1\":[], \"accuracy\":[]}\n",
    "    for ftr in [\"Actor\", \"InstrumentType\", \"Objective\", \"Resource\", \"Time\"]:\n",
    "        results_dict[model_name][ftr] = {\"f1\":[]}\n",
    "for model_name in list(results_dict):\n",
    "    for r in [0,1,2]:\n",
    "        dataset_dict = DatasetDict.load_from_disk(cwd+f\"/inputs/{mode}/dsdct_r{r}\")\n",
    "        config = MultiHeadTokenConfig.from_pretrained(cwd + f\"/models/{mode}/{model_name.split('/')[-1]}_{r}\")\n",
    "        model_tt = DebertaForMultiHeadTokClass(config)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenized_dsdct = dataset_dict.map(tokenize_and_align_labels, batched=True)\n",
    "        model_tt.to('cuda')\n",
    "        model_tt.eval()\n",
    "        texts = list(tokenized_dsdct['test']['text'])\n",
    "        # tokenize batch\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, is_split_into_words=False)\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "        # get labels in batch form\n",
    "        label_cols = [\"labels_Actor\",\"labels_InstrumentType\",\"labels_Objective\",\"labels_Resource\",\"labels_Time\"]\n",
    "        labels_batch = {col: torch.tensor(tokenized_dsdct['test'][col]).to('cuda') for col in label_cols}\n",
    "        with torch.no_grad():\n",
    "            model_inputs = {\n",
    "                \"input_ids\": inputs[\"input_ids\"].to('cuda'),\n",
    "                \"attention_mask\": inputs[\"attention_mask\"].to('cuda')\n",
    "            }\n",
    "            outputs = model_tt(**model_inputs, **labels_batch)\n",
    "            # outputs is TokenClassifierOutput\n",
    "            logits = outputs.logits  # dict of logits per head\n",
    "        results = compute_metrics_multihead_res((logits, labels_batch))\n",
    "        print(results)\n",
    "        for k, v in results.items():\n",
    "            for mtr in v:\n",
    "                results_dict[model_name][k][mtr].append(float(v[mtr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063cd5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in list(results_dict):\n",
    "    print(f\"\\n{m}\")\n",
    "    for res in list(results_dict[m]):\n",
    "        print(f\"{res}\")\n",
    "        print(results_dict[m][res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126ee284",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"2nd_results_separateheads_tokenmicrof1\"\n",
    "with open(cwd+f\"/outputs/{fn}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30426c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in list(results_dict):\n",
    "    print(f\"\\n{m}\")\n",
    "    for res in list(results_dict[m]):\n",
    "        print(f\"\\n{res}\")\n",
    "        df = pd.DataFrame(results_dict[m][res])\n",
    "        df.loc['mean'] = df.mean()\n",
    "        #print(df)\n",
    "        print(round(df.loc['mean']*100,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
