{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a2a5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from ner_eval import tokf1_calc\n",
    "from collections import Counter\n",
    "from seqeval.metrics import classification_report as sqclassification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report as skclassification_report\n",
    "import evaluate\n",
    "\n",
    "from datasets import DatasetDict\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "from sghead_ner_ft import SgheadDataset, sghead_collate, sghead_evaluate_model\n",
    "from mhead_ner_ft import MheadDataset, MheadTokenClassifier, mhead_collate, mhead_evaluate_model\n",
    "from create_datasets import get_label_set\n",
    "\n",
    "from nervaluate import Evaluator\n",
    "import torch\n",
    "\n",
    "cwd = os.getcwd()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f645bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"a\"\n",
    "htype = \"mhead\"\n",
    "model_name = \"FacebookAI/xlm-roberta-base\" #[\"microsoft/deberta-v3-base\",\"FacebookAI/xlm-roberta-base\",\"dslim/bert-base-NER-uncased\"]:\n",
    "r=2\n",
    "dsdcts_dir = f\"{cwd}/inputs/{mode}/{htype}_dsdcts\"\n",
    "models_dir = f\"{cwd}/models/{mode}/{htype}\"\n",
    "results_dir = f\"{cwd}/results/{mode}/{htype}/\"\n",
    "model_addr = f\"{models_dir}/{model_name.split('/')[-1]}_{r}\"\n",
    "randp_fn = f\"{results_dir}/randp_{model_name.split('/')[-1]}_{r}.json\"\n",
    "dev = \"cuda\"\n",
    "label_list = get_label_set(mode, \"mhead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f34896c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict.load_from_disk(f\"{dsdcts_dir}/dsdct_r{r}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if htype==\"sghead\":\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_addr).to(dev)\n",
    "if htype==\"mhead\":\n",
    "    dropout=0.1\n",
    "    head_lst = get_label_set(mode, htype)\n",
    "    model = MheadTokenClassifier(model_name, head_lst, dropout = dropout).to(dev)\n",
    "    model.load_state_dict(torch.load(model_addr+\"/model.pt\", weights_only=True))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aca7528",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(randp_fn, \"r\", encoding=\"utf-8\") as f:\n",
    "    randp = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a42ead",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m seqeval \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseqeval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m all_reals, all_preds \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m      3\u001b[0m sk_reals, sk_preds \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "all_reals, all_preds = [], []\n",
    "sk_reals, sk_preds = [], []\n",
    "sq_reals, sq_preds = [], []\n",
    "for head in list(randp):\n",
    "    for art in randp[head]:\n",
    "        reals = [ent[0] for ent in art]\n",
    "        preds = [ent[1] for ent in art]\n",
    "        all_reals += reals\n",
    "        all_preds += preds\n",
    "        #\n",
    "        reals = [ent[0] for ent in art]\n",
    "        preds = [ent[1] for ent in art]\n",
    "        sq_reals.append(reals)\n",
    "        sq_preds.append(preds)\n",
    "    res = seqeval.compute(predictions=sq_preds, references=sq_reals)\n",
    "    print(head, res)\n",
    "    evaluator = Evaluator(sq_reals, sq_preds, tags='BIO', loader=\"list\")\n",
    "    print(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8789ed4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Actor': {'micro_f1': 1.0, 'macro_f1': 1.0, 'weighted_f1': 1.0}, 'InstrumentType': {'micro_f1': 1.0, 'macro_f1': 1.0, 'weighted_f1': 1.0}, 'Objective': {'micro_f1': 1.0, 'macro_f1': 1.0, 'weighted_f1': 1.0}, 'Resource': {'micro_f1': 0.9194630872483222, 'macro_f1': 0.31934731934731936, 'weighted_f1': 0.8808842164546862}, 'Time': {'micro_f1': 1.0, 'macro_f1': 1.0, 'weighted_f1': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "tokf1 = tokf1_calc(htype, randp)\n",
    "print(tokf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32edb858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.92      0.94      0.93       116\n",
      "           I       0.91      0.91      0.91       138\n",
      "           O       1.00      1.00      1.00      3288\n",
      "\n",
      "    accuracy                           0.99      3542\n",
      "   macro avg       0.94      0.95      0.95      3542\n",
      "weighted avg       0.99      0.99      0.99      3542\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   0.94      0.94      0.94       254\n",
      "           O       1.00      1.00      1.00      3288\n",
      "\n",
      "    accuracy                           0.99      3542\n",
      "   macro avg       0.97      0.97      0.97      3542\n",
      "weighted avg       0.99      0.99      0.99      3542\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.90      0.91      0.90       117\n",
      "\n",
      "   micro avg       0.90      0.91      0.90       117\n",
      "   macro avg       0.90      0.91      0.90       117\n",
      "weighted avg       0.90      0.91      0.90       117\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.84      0.84      0.84       167\n",
      "           I       0.87      0.73      0.79       203\n",
      "           O       0.99      0.99      0.99      6714\n",
      "\n",
      "    accuracy                           0.98      7084\n",
      "   macro avg       0.90      0.86      0.88      7084\n",
      "weighted avg       0.98      0.98      0.98      7084\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   0.89      0.81      0.85       370\n",
      "           O       0.99      0.99      0.99      6714\n",
      "\n",
      "    accuracy                           0.98      7084\n",
      "   macro avg       0.94      0.90      0.92      7084\n",
      "weighted avg       0.98      0.98      0.98      7084\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.79      0.79      0.79       168\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       168\n",
      "   macro avg       0.79      0.79      0.79       168\n",
      "weighted avg       0.79      0.79      0.79       168\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.79      0.81      0.80       174\n",
      "           I       0.86      0.59      0.70       297\n",
      "           O       0.99      1.00      0.99     10155\n",
      "\n",
      "    accuracy                           0.98     10626\n",
      "   macro avg       0.88      0.80      0.83     10626\n",
      "weighted avg       0.98      0.98      0.98     10626\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   0.87      0.70      0.78       471\n",
      "           O       0.99      1.00      0.99     10155\n",
      "\n",
      "    accuracy                           0.98     10626\n",
      "   macro avg       0.93      0.85      0.88     10626\n",
      "weighted avg       0.98      0.98      0.98     10626\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.74      0.75      0.75       175\n",
      "\n",
      "   micro avg       0.74      0.75      0.75       175\n",
      "   macro avg       0.74      0.75      0.75       175\n",
      "weighted avg       0.74      0.75      0.75       175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.79      0.77      0.78       183\n",
      "           I       0.86      0.58      0.69       305\n",
      "           O       0.99      1.00      0.99     13680\n",
      "\n",
      "    accuracy                           0.98     14168\n",
      "   macro avg       0.88      0.78      0.82     14168\n",
      "weighted avg       0.98      0.98      0.98     14168\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   0.87      0.68      0.76       488\n",
      "           O       0.99      1.00      0.99     13680\n",
      "\n",
      "    accuracy                           0.99     14168\n",
      "   macro avg       0.93      0.84      0.88     14168\n",
      "weighted avg       0.98      0.99      0.98     14168\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.74      0.72      0.73       184\n",
      "\n",
      "   micro avg       0.74      0.72      0.73       184\n",
      "   macro avg       0.74      0.72      0.73       184\n",
      "weighted avg       0.74      0.72      0.73       184\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.79      0.78      0.79       194\n",
      "           I       0.85      0.62      0.72       343\n",
      "           O       0.99      1.00      0.99     17173\n",
      "\n",
      "    accuracy                           0.99     17710\n",
      "   macro avg       0.87      0.80      0.83     17710\n",
      "weighted avg       0.99      0.99      0.99     17710\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   0.85      0.71      0.78       537\n",
      "           O       0.99      1.00      0.99     17173\n",
      "\n",
      "    accuracy                           0.99     17710\n",
      "   macro avg       0.92      0.85      0.88     17710\n",
      "weighted avg       0.99      0.99      0.99     17710\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.74      0.73      0.74       195\n",
      "\n",
      "   micro avg       0.74      0.73      0.74       195\n",
      "   macro avg       0.74      0.73      0.74       195\n",
      "weighted avg       0.74      0.73      0.74       195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_reals, all_preds = [], []\n",
    "sk_reals, sk_preds = [], []\n",
    "sq_reals, sq_preds = [], []\n",
    "for art in randp:\n",
    "    reals = [ent[0] for ent in art]\n",
    "    preds = [ent[1] for ent in art]\n",
    "    all_reals += reals\n",
    "    all_preds += preds\n",
    "    #\n",
    "    reals = [ent[0][2:] if ent[0]!=\"O\" else \"O\" for ent in art]\n",
    "    preds = [ent[1][2:] if ent[1]!=\"O\" else \"O\" for ent in art]\n",
    "    sk_reals += reals\n",
    "    sk_preds += preds\n",
    "    #\n",
    "    reals = [ent[0] for ent in art]\n",
    "    preds = [ent[1] for ent in art]\n",
    "    sq_reals.append(reals)\n",
    "    sq_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1ea1a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marwas/miniconda3/envs/kgenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/marwas/miniconda3/envs/kgenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/marwas/miniconda3/envs/kgenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/marwas/miniconda3/envs/kgenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/marwas/miniconda3/envs/kgenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/marwas/miniconda3/envs/kgenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/marwas/miniconda3/envs/kgenv/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         B-Actor       0.85      0.90      0.88      1334\n",
      "B-InstrumentType       0.55      0.62      0.58       663\n",
      "     B-Objective       0.24      0.31      0.27       138\n",
      "      B-Resource       0.00      0.00      0.00       119\n",
      "          B-Time       0.45      0.59      0.51       178\n",
      "         I-Actor       0.87      0.80      0.83      1654\n",
      "I-InstrumentType       0.59      0.50      0.54       775\n",
      "     I-Objective       0.59      0.29      0.39       919\n",
      "      I-Resource       0.00      0.00      0.00       166\n",
      "          I-Time       0.71      0.65      0.68       546\n",
      "               O       0.97      0.98      0.98     70154\n",
      "\n",
      "        accuracy                           0.95     76646\n",
      "       macro avg       0.53      0.51      0.51     76646\n",
      "    weighted avg       0.95      0.95      0.95     76646\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         Actor       0.90      0.88      0.89      2988\n",
      "InstrumentType       0.64      0.63      0.63      1438\n",
      "             O       0.97      0.98      0.98     70154\n",
      "     Objective       0.58      0.35      0.44      1057\n",
      "      Resource       0.00      0.00      0.00       285\n",
      "          Time       0.70      0.71      0.71       724\n",
      "\n",
      "      accuracy                           0.96     76646\n",
      "     macro avg       0.63      0.59      0.61     76646\n",
      "  weighted avg       0.95      0.96      0.96     76646\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         Actor       0.79      0.84      0.81      1336\n",
      "InstrumentType       0.50      0.56      0.53       668\n",
      "     Objective       0.17      0.19      0.18       164\n",
      "      Resource       0.00      0.00      0.00       121\n",
      "          Time       0.35      0.46      0.40       179\n",
      "\n",
      "     micro avg       0.62      0.65      0.64      2468\n",
      "     macro avg       0.36      0.41      0.38      2468\n",
      "  weighted avg       0.60      0.65      0.62      2468\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sk1 = skclassification_report(all_reals, all_preds)\n",
    "sk2 = skclassification_report(sk_reals, sk_preds)\n",
    "sq = sqclassification_report(sq_reals, sq_preds)\n",
    "print(sk1)\n",
    "print(sk2)\n",
    "print(sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73a63ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marwas/miniconda3/envs/kgenv/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Actor': {'precision': np.float64(0.9001026342798495), 'recall': np.float64(0.8805220883534136), 'f1': np.float64(0.8902047030959228), 'number': np.int64(2988)}, 'InstrumentType': {'precision': np.float64(0.6365561044460127), 'recall': np.float64(0.627260083449235), 'f1': np.float64(0.6318739054290718), 'number': np.int64(1438)}, 'Objective': {'precision': np.float64(0.5799373040752351), 'recall': np.float64(0.3500473036896878), 'f1': np.float64(0.43657817109144537), 'number': np.int64(1057)}, 'Resource': {'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1': np.float64(0.0), 'number': np.int64(285)}, 'Time': {'precision': np.float64(0.7012278308321964), 'recall': np.float64(0.7099447513812155), 'f1': np.float64(0.7055593685655457), 'number': np.int64(724)}, 'overall_precision': np.float64(0.7734197163368937), 'overall_recall': np.float64(0.6803758471965496), 'overall_f1': np.float64(0.7239203474555437), 'overall_accuracy': 0.9542572345588811}\n"
     ]
    }
   ],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "metrics = seqeval.compute(predictions=sq_preds, references=sq_reals)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc4f53c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'micro_f1': 0.9542572345588811, 'macro_f1': 0.5149809982711112, 'weighted_f1': 0.950775895964328}\n"
     ]
    }
   ],
   "source": [
    "mtcs = {\n",
    "    \"micro_f1\":f1_score(all_reals, all_preds, average='micro'),\n",
    "    \"macro_f1\":f1_score(all_reals, all_preds, average=\"macro\"),\n",
    "    \"weighted_f1\":f1_score(all_reals, all_preds, average=\"weighted\")\n",
    "}\n",
    "print(mtcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cca863c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds, new_reals = [], []\n",
    "for nart in range(len(sq_reals)):\n",
    "    new_preds.append(\" \".join(sq_preds[nart]))\n",
    "    new_reals.append(\" \".join(sq_reals[nart]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03fae58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_calc(htype, randp):\n",
    "    if htype == \"sghead\":\n",
    "        all_reals = []\n",
    "        all_preds = []\n",
    "        for art in randp:\n",
    "            reals = [ent[0] for ent in art]\n",
    "            preds = [ent[1] for ent in art]\n",
    "            all_reals += reals\n",
    "            all_preds += preds\n",
    "        scores = {\n",
    "            \"sklearn\":skclassification_report(all_reals, all_preds),\n",
    "            \"seqeval\":sqclassification_report(all_reals, all_preds)\n",
    "        }\n",
    "        return scores\n",
    "    elif htype == \"mhead\":\n",
    "        label_lst = list(randp)\n",
    "        scores_dct = {head:{} for head in label_lst}\n",
    "        for head in label_lst:\n",
    "            all_reals = []\n",
    "            all_preds = []\n",
    "            for art in randp[head]:\n",
    "                reals = [ent[0] for ent in art]\n",
    "                preds = [ent[1] for ent in art]\n",
    "                all_reals += reals\n",
    "                all_preds += preds\n",
    "            scores_dct[head] = {\n",
    "                \"sklearn\":skclassification_report(all_reals, all_preds),\n",
    "                \"seqeval\":sqclassification_report(all_reals, all_preds)\n",
    "            }\n",
    "        return scores_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0950e58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marwas/miniconda3/envs/kgenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/marwas/miniconda3/envs/kgenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/marwas/miniconda3/envs/kgenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Found input variables without list of list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmetric_calc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics)\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mmetric_calc\u001b[0;34m(htype, randp)\u001b[0m\n\u001b[1;32m      8\u001b[0m         all_reals \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reals\n\u001b[1;32m      9\u001b[0m         all_preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m preds\n\u001b[1;32m     10\u001b[0m     scores \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn\u001b[39m\u001b[38;5;124m\"\u001b[39m:skclassification_report(all_reals, all_preds),\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseqeval\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[43msqclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_reals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     }\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m htype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmhead\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/kgenv/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:692\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, digits, suffix, output_dict, mode, sample_weight, zero_division, scheme)\u001b[0m\n\u001b[1;32m    689\u001b[0m     reporter \u001b[38;5;241m=\u001b[39m StringReporter(width\u001b[38;5;241m=\u001b[39mwidth, digits\u001b[38;5;241m=\u001b[39mdigits)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;66;03m# compute per-class scores.\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m p, r, f1, s \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target_names, p, r, f1, s):\n\u001b[1;32m    700\u001b[0m     reporter\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;241m*\u001b[39mrow)\n",
      "File \u001b[0;32m~/miniconda3/envs/kgenv/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:130\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, average, warn_for, beta, sample_weight, zero_division, suffix)\u001b[0m\n\u001b[1;32m    126\u001b[0m         true_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(true_sum, \u001b[38;5;28mlen\u001b[39m(entities_true_type))\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pred_sum, tp_sum, true_sum\n\u001b[0;32m--> 130\u001b[0m precision, recall, f_score, true_sum \u001b[38;5;241m=\u001b[39m \u001b[43m_precision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarn_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheme\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_tp_actual_correct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_tp_actual_correct\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m precision, recall, f_score, true_sum\n",
      "File \u001b[0;32m~/miniconda3/envs/kgenv/lib/python3.10/site-packages/seqeval/metrics/v1.py:122\u001b[0m, in \u001b[0;36m_precision_recall_fscore_support\u001b[0;34m(y_true, y_pred, average, warn_for, beta, sample_weight, zero_division, scheme, suffix, extract_tp_actual_correct)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(average_options))\n\u001b[0;32m--> 122\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m pred_sum, tp_sum, true_sum \u001b[38;5;241m=\u001b[39m extract_tp_actual_correct(y_true, y_pred, suffix, scheme)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/kgenv/lib/python3.10/site-packages/seqeval/metrics/v1.py:97\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     95\u001b[0m is_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtype\u001b[39m, y_true)) \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtype\u001b[39m, y_pred))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list \u001b[38;5;241m==\u001b[39m {\u001b[38;5;28mlist\u001b[39m}:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound input variables without list of list.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_true) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_pred) \u001b[38;5;129;01mor\u001b[39;00m len_true \u001b[38;5;241m!=\u001b[39m len_pred:\n\u001b[1;32m    100\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(len_true, len_pred)\n",
      "\u001b[0;31mTypeError\u001b[0m: Found input variables without list of list."
     ]
    }
   ],
   "source": [
    "metrics = metric_calc(htype, randp)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
